{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "bFWbEb6uGbN-"
      },
      "source": [
        "# Next-Word Prediction Using Long Short-Term Memory (LSTM) Networks\n",
        "\n",
        "Implement the model that will predict the next word in a text sequence and train it using a corpus of [Shakespeare Sonnets](https://www.opensourceshakespeare.org/views/sonnets/sonnet_view.php?range=viewrange&sonnetrange1=1&sonnetrange2=154)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "BOwsuGQQY9OL",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "F6WZIO2WA7iT"
      },
      "source": [
        "## Defining global variables\n",
        "\n",
        "Define some global variables that will be used throughout the project.\n",
        "\n",
        "- `FILE_PATH`: The file path where the sonnets file is located.\n",
        "\n",
        "- `NUM_BATCHES`: Number of batches. Defaults to 16.\n",
        "- `LSTM_UNITS`: Number of LSTM units in the LSTM layer.\n",
        "- `EMBEDDING_DIM`: Number of dimensions in the embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "LEnByWiyA7iT"
      },
      "outputs": [],
      "source": [
        "FILE_PATH = 'sonnets1.txt'\n",
        "NUM_BATCHES = 16\n",
        "LSTM_UNITS = 128\n",
        "EMBEDDING_DIM = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "by7ULGrsA7iU"
      },
      "source": [
        "### Reading the dataset\n",
        "\n",
        "For this project you will be using the [Shakespeare Sonnets Dataset](https://www.opensourceshakespeare.org/views/sonnets/sonnet_view.php?range=viewrange&sonnetrange1=1&sonnetrange2=154), which contains more than 2000 lines of text extracted from Shakespeare's sonnets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Pfd-nYKij5yY"
      },
      "outputs": [],
      "source": [
        "# Read the data\n",
        "with open(FILE_PATH) as f:\n",
        "    data = f.read()\n",
        "\n",
        "# Convert to lower case and save as a list\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "print(f\"There are {len(corpus)} lines of sonnets\\n\")\n",
        "print(f\"The first 5 lines look like this:\\n\")\n",
        "for i in range(5):\n",
        "  print(corpus[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "imB15zrSNhA1"
      },
      "source": [
        "## 1: fit_vectorizer\n",
        "\n",
        "You will use the [tf.keras.layers.TextVectorization layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) to tokenize and transform the text into numeric values.\n",
        "\n",
        "**Note**:\n",
        "- You should remove the punctuation and use only lowercase words, so you must pass the correct argument to TextVectorization layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "G7eAwEB_A7iV"
      },
      "outputs": [],
      "source": [
        "def fit_vectorizer(corpus):\n",
        "    \"\"\"\n",
        "    Instantiates the vectorizer class on the corpus\n",
        "\n",
        "    Args:\n",
        "        corpus (list): List with the sentences.\n",
        "\n",
        "    Returns:\n",
        "        (tf.keras.layers.TextVectorization): an instance of the TextVectorization class containing the word-index dictionary, adapted to the corpus sentences.\n",
        "    \"\"\"\n",
        "\n",
        "    tf.keras.utils.set_random_seed(65) # Do not change this line or you may have different expected outputs throughout the assignment\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "     # Define the object with appropriate parameters\n",
        "    vectorizer =                (\n",
        "        standardize=                  # Convert to lowercase and strip punctuation\n",
        "        split=                        # Split on whitespace (default)\n",
        "        ragged=                       # Allow ragged tensors\n",
        "        output_mode=                  # Output as integers\n",
        "    )\n",
        "\n",
        "    # Adapt it to the corpus\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "vus32Jp9A7iV"
      },
      "outputs": [],
      "source": [
        "vectorizer = fit_vectorizer(corpus)\n",
        "total_words = len(vectorizer.get_vocabulary())\n",
        "print('Name:                 Register Number:                 ')\n",
        "print(f\"Total number of words in corpus (including the out of vocabulary): {total_words}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77-0sA46OETa"
      },
      "source": [
        "One thing to note is that you can either pass a string or a list of strings to vectorizer. If you pass the former, it will return a *tensor* whereas if you pass the latter, it will return a *ragged tensor* if you've correctly configured the TextVectorization layer to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "tqhPxdeXlfjh"
      },
      "outputs": [],
      "source": [
        "print('Name:                 Register Number:                 ')\n",
        "print(f\"Passing a string directly: {vectorizer('This is a test string').__repr__()}\")\n",
        "print(f\"Passing a list of strings: {vectorizer(['This is a test string'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "-oqy9KjXRJ9A"
      },
      "source": [
        "## Generating n-grams\n",
        "\n",
        "## 2: n_grams_seqs\n",
        "\n",
        "Complete the `n_gram_seqs` function below. This function receives the fitted vectorizer and the corpus (which is a list of strings) and should return a list containing the `n_gram` sequences for each line in the corpus.\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "- If you pass `vectorizer(sentence)` the result is not padded, whereas if you pass `vectorizer(list_of_sentences)`, the result won't be padded **only if you passed the argument `ragged = True`** in the TextVectorization setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "iy4baJMDl6kj",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def n_gram_seqs(corpus, vectorizer):\n",
        "    \"\"\"\n",
        "    Generates a list of n-gram sequences\n",
        "\n",
        "    Args:\n",
        "        corpus (list of string): lines of texts to generate n-grams for\n",
        "        vectorizer (tf.keras.layers.TextVectorization): an instance of the TextVectorization class adapted in the corpus\n",
        "\n",
        "    Returns:\n",
        "        (list of tf.int64 tensors): the n-gram sequences for each line in the corpus\n",
        "    \"\"\"\n",
        "    input_sequences = []\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    for sentence in corpus:\n",
        "        # Vectorize the sentence to get the token indices\n",
        "        vectorized_sentence =\n",
        "\n",
        "        # Generate n-grams for the vectorized sentence\n",
        "        for i in range( ):  # Start from 2 to avoid the first token\n",
        "            n_gram =\n",
        "            input_sequences.append( )\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return input_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "dx3V_RjFWQSu"
      },
      "source": [
        "Apply the `n_gram_seqs` transformation to the whole corpus and save the maximum sequence length to use it later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "laMwiRUpmuSd"
      },
      "outputs": [],
      "source": [
        "# Apply the n_gram_seqs transformation to the whole corpus\n",
        "input_sequences = n_gram_seqs(corpus, vectorizer)\n",
        "\n",
        "# Save max length\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "print('Name:                 Register Number:                 ')\n",
        "print(f\"n_grams of input_sequences have length: {len(input_sequences)}\")\n",
        "print(f\"maximum length of sequences is: {max_sequence_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "zHY7HroqWq12"
      },
      "source": [
        "## 3: pad_seqs\n",
        "\n",
        "Now code the `pad_seqs` function which will pad any given sequences to the desired maximum length. Notice that this function receives a list of sequences and should return a numpy array with the padded sequences. You may have a look at the documentation of [`tf.keras.utils.pad_sequences`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "deletable": false,
        "id": "WW1-qAZaWOhC",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def pad_seqs(input_sequences, max_sequence_len):\n",
        "    \"\"\"\n",
        "    Pads tokenized sequences to the same length\n",
        "\n",
        "    Args:\n",
        "        input_sequences (list of int): tokenized sequences to pad\n",
        "        maxlen (int): maximum length of the token sequences\n",
        "\n",
        "    Returns:\n",
        "        (np.array of int32): tokenized sequences padded to the same length\n",
        "    \"\"\"\n",
        "\n",
        "   ### START CODE HERE ###\n",
        "    # Convert tensors to lists if necessary\n",
        "    input_list = [seq if isinstance(seq, list) else seq.numpy().tolist() for seq in input_sequences]\n",
        "\n",
        "    # Use pad_sequences to pad the sequences with left padding ('pre')\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                     # Use the list of lists for padding\n",
        "                     # Set the maximum length\n",
        "                     # Pad sequences to the left (before the sequence)\n",
        "                     # Specify the output type as int32\n",
        "    )\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return padded_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "rgK-Q_micEYA"
      },
      "outputs": [],
      "source": [
        "# Pad the whole corpus\n",
        "input_sequences = pad_seqs(input_sequences, max_sequence_len)\n",
        "print('Name:                 Register Number:                 ')\n",
        "print(f\"padded corpus has shape: {input_sequences.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ZbOidyPrXxf7"
      },
      "source": [
        "## 4: features_and_labels_dataset\n",
        "\n",
        "Before feeding the data into the neural network you should split it into features and labels. In this case the features will be the *padded n_gram sequences* with the **last element** removed from them and the labels will be the removed words.\n",
        "\n",
        "Complete the `features_and_labels_dataset` function below. This function expects the `padded n_gram sequences` as input and should return a **batched** [tensorflow dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) containing elements in the form (sentence, label).\n",
        "\n",
        "\n",
        "**NOTE**:\n",
        "- Notice that the function also receives the total of words in the corpus, this parameter will be **very important when one hot encoding the labels** since every word in the corpus will be a label at least once. The function you should use is [`tf.keras.utils.to_categorical`]((https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)).\n",
        "- To generate a dataset you may use the function [tf.data.Dataset.from_tensor_slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) after obtaining the sentences and their respective labels.\n",
        "- To batch a dataset, you may call the method [.batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch). A good number is `16`, but feel free to choose any number you want to, but keep it not greater than 64, otherwise the model may take too many epochs to achieve a good accuracy. Remember this is defined as a global variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "deletable": false,
        "id": "9WGGbYdnZdmJ",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def features_and_labels_dataset(input_sequences, total_words):\n",
        "    \"\"\"\n",
        "    Generates features and labels from n-grams and returns a tensorflow dataset\n",
        "\n",
        "    Args:\n",
        "        input_sequences (list of int): sequences to split features and labels from\n",
        "        total_words (int): vocabulary size\n",
        "\n",
        "    Returns:\n",
        "        (tf.data.Dataset): Dataset with elements in the form (sentence, label)\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    # Define the features by taking all tokens except the last one for each sequence\n",
        "    features =\n",
        "\n",
        "    # Define the labels by taking the last token for each sequence\n",
        "    labels =\n",
        "\n",
        "    # One-hot encode the labels using total_words as the number of classes\n",
        "    one_hot_labels =\n",
        "\n",
        "    # Build the dataset using the features and one-hot encoded labels\n",
        "    dataset =\n",
        "\n",
        "    # Batch the dataset with a batch size of 16\n",
        "    batch_size = 16  # Feel free to adjust this based on the global variable, but should be <= 64\n",
        "    batched_dataset = dataset.batch(batch_size)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return batched_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "GRTuLEt3bRKa"
      },
      "outputs": [],
      "source": [
        "# Split the whole corpus\n",
        "dataset = features_and_labels_dataset(input_sequences, total_words).prefetch(tf.data.AUTOTUNE)\n",
        "print('Name:                 Register Number:                 ')\n",
        "print(f\"Feature shape: {dataset.element_spec[0]}\")\n",
        "print(f\"Label shape: {dataset.element_spec[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ltxaOCE_aU6J"
      },
      "source": [
        "## 5: create_model\n",
        "\n",
        "Now you should define a model architecture capable of achieving an accuracy of at least 80%.\n",
        "\n",
        "Some hints to help you in this task:\n",
        "\n",
        "- The first layer in your model must be an [Input](https://www.tensorflow.org/api_docs/python/tf/keras/Input) layer with the appropriate parameters, remember that your input are vectors with a fixed length size. Be careful with the size value you should pass as you've removed the last element of every input to be the label.\n",
        "\n",
        "- An appropriate `output_dim` for the first layer (Embedding) is 100, this is already provided for you.\n",
        "- A Bidirectional LSTM is helpful for this particular problem.\n",
        "- The last layer should have the same number of units as the total number of words in the corpus and a softmax activation function.\n",
        "- This problem can be solved with only two layers (excluding the Embedding and Input) so try out small architectures first.\n",
        "- 30 epochs should be enough to get an accuracy higher than 80%, if this is not the case try changing the architecture of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "code",
        "deletable": false,
        "id": "XrE6kpJFfvRY",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def create_model(total_words, max_sequence_len):\n",
        "    \"\"\"\n",
        "    Creates a text generator model\n",
        "\n",
        "    Args:\n",
        "        total_words (int): size of the vocabulary for the Embedding layer input\n",
        "        max_sequence_len (int): length of the input sequences\n",
        "\n",
        "    Returns:\n",
        "       (tf.keras Model): the text generator model\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "   ### START CODE HERE ###\n",
        "    # Input layer shape is max_sequence_len - 1 because we removed the last word as a label\n",
        "    model.add()\n",
        "\n",
        "    # Embedding layer\n",
        "    model.add()\n",
        "\n",
        "    # Add a Bidirectional LSTM layer with 150 units\n",
        "    model.add()\n",
        "\n",
        "    # Add a Dense layer with 'total_words' units and softmax activation\n",
        "    model.add()\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile()\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "0IpX_Gu_gISk",
        "tags": [],
        "outputId": "ba241c29-dbe7-4dc7-c0cd-08399e5433d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Get the untrained model\n",
        "model = create_model(total_words, max_sequence_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "sWi5Sue_A7ie"
      },
      "outputs": [],
      "source": [
        "example_batch = dataset.take(1)\n",
        "\n",
        "try:\n",
        "\tmodel.evaluate(example_batch, verbose=False)\n",
        "except:\n",
        "\tprint(\"Your model is not compatible with the dataset you defined earlier. Check that the loss function and last layer are compatible with one another.\")\n",
        "else:\n",
        "\tpredictions = model.predict(example_batch, verbose=False)\n",
        "\tprint(f\"predictions have shape: {predictions.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "Y-xWcitpA7ie"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(dataset, epochs=30, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "gy72RPgly55q"
      },
      "source": [
        "**Your model should achieve a training accuracy of at least 80%**. If your model didn't achieve this threshold, try training again with a different model architecture. Consider increasing the number of units in your `LSTM` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1fXTEO3GJ282",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Get training and validation accuracies\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "# Get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "fig.suptitle('***Training performance - Accuracy and Loss*** Name:         Register Number:        ')\n",
        "\n",
        "for i, (data, label) in enumerate(zip([acc,loss], [\"Accuracy\", \"Loss\"])):\n",
        "    ax[i].plot(epochs, data, label=label)\n",
        "    ax[i].legend()\n",
        "    ax[i].set_xlabel('epochs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "OjvED5A3qrn2"
      },
      "source": [
        "If the accuracy meets the requirement of being greater than 80%, then save the `history.pkl` file which contains the information of the training history of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "9QRG73l6qE-c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "with open('history.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "wdsMszk9zBs_"
      },
      "source": [
        "## See your model in action\n",
        "\n",
        "After all your work it is finally time to see your model generating text.\n",
        "\n",
        "Run the cell below to generate the next 100 words of a seed text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "6Vc6PHgxa6Hm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "\n",
        "for _ in range(next_words):\n",
        "    # Convert the text into sequences\n",
        "    token_list = vectorizer(seed_text)\n",
        "    # Pad the sequences\n",
        "    token_list = tf.keras.utils.pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    # Get the probabilities of predicting a word\n",
        "    predicted = model.predict([token_list], verbose=0)\n",
        "    # Choose the next word based on the maximum probability\n",
        "    predicted = np.argmax(predicted, axis=-1).item()\n",
        "    # Get the actual word from the word index\n",
        "    output_word = vectorizer.get_vocabulary()[predicted]\n",
        "    # Append to the current text\n",
        "    seed_text += \" \" + output_word\n",
        "print('Name:              Register Number:          ')\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "dlai_version": "1.2.0",
    "grader_version": "1",
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}